{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os \n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory \n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\hp\\\\Desktop\\\\PProject\\\\End-to-End-Source-code-Analysis-GenAi\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file test_repo already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "GitCommandError",
     "evalue": "Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v -- https://github.com/rohitmukati/End-to-End-Medical-Chatbot-Generative-AI test_repo/\n  stderr: 'fatal: destination path 'test_repo' already exists and is not an empty directory.\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mGitCommandError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m repo_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_repo/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m repo \u001b[38;5;241m=\u001b[39m \u001b[43mRepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone_from\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://github.com/rohitmukati/End-to-End-Medical-Chatbot-Generative-AI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\genai_2\\lib\\site-packages\\git\\repo\\base.py:1525\u001b[0m, in \u001b[0;36mRepo.clone_from\u001b[1;34m(cls, url, to_path, progress, env, multi_options, allow_unsafe_protocols, allow_unsafe_options, **kwargs)\u001b[0m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1524\u001b[0m     git\u001b[38;5;241m.\u001b[39mupdate_environment(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menv)\n\u001b[1;32m-> 1525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_clone(\n\u001b[0;32m   1526\u001b[0m     git,\n\u001b[0;32m   1527\u001b[0m     url,\n\u001b[0;32m   1528\u001b[0m     to_path,\n\u001b[0;32m   1529\u001b[0m     GitCmdObjectDB,\n\u001b[0;32m   1530\u001b[0m     progress,\n\u001b[0;32m   1531\u001b[0m     multi_options,\n\u001b[0;32m   1532\u001b[0m     allow_unsafe_protocols\u001b[38;5;241m=\u001b[39mallow_unsafe_protocols,\n\u001b[0;32m   1533\u001b[0m     allow_unsafe_options\u001b[38;5;241m=\u001b[39mallow_unsafe_options,\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1535\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\genai_2\\lib\\site-packages\\git\\repo\\base.py:1396\u001b[0m, in \u001b[0;36mRepo._clone\u001b[1;34m(cls, git, url, path, odb_default_type, progress, multi_options, allow_unsafe_protocols, allow_unsafe_options, **kwargs)\u001b[0m\n\u001b[0;32m   1393\u001b[0m     cmdline \u001b[38;5;241m=\u001b[39m remove_password_if_present(cmdline)\n\u001b[0;32m   1395\u001b[0m     _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCmd(\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms unused stdout: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, cmdline, stdout)\n\u001b[1;32m-> 1396\u001b[0m     \u001b[43mfinalize_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstderr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1398\u001b[0m \u001b[38;5;66;03m# Our git command could have a different working dir than our actual\u001b[39;00m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;66;03m# environment, hence we prepend its working dir if required.\u001b[39;00m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m osp\u001b[38;5;241m.\u001b[39misabs(path):\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\genai_2\\lib\\site-packages\\git\\util.py:504\u001b[0m, in \u001b[0;36mfinalize_process\u001b[1;34m(proc, **kwargs)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for the process (clone, fetch, pull or push) and handle its errors\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;124;03maccordingly.\"\"\"\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;66;03m# TODO: No close proc-streams??\u001b[39;00m\n\u001b[1;32m--> 504\u001b[0m proc\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\genai_2\\lib\\site-packages\\git\\cmd.py:834\u001b[0m, in \u001b[0;36mGit.AutoInterrupt.wait\u001b[1;34m(self, stderr)\u001b[0m\n\u001b[0;32m    832\u001b[0m     errstr \u001b[38;5;241m=\u001b[39m read_all_from_possibly_closed_stream(p_stderr)\n\u001b[0;32m    833\u001b[0m     _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoInterrupt wait stderr: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (errstr,))\n\u001b[1;32m--> 834\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GitCommandError(remove_password_if_present(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs), status, errstr)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m status\n",
      "\u001b[1;31mGitCommandError\u001b[0m: Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v -- https://github.com/rohitmukati/End-to-End-Medical-Chatbot-Generative-AI test_repo/\n  stderr: 'fatal: destination path 'test_repo' already exists and is not an empty directory.\n'"
     ]
    }
   ],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "repo = Repo.clone_from(\"https://github.com/rohitmukati/End-to-End-Medical-Chatbot-Generative-AI\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                       glob = \"**/*\",\n",
    "                                       suffixes = [\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    "                                       \n",
    "                                       \n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_huggingface_embedding\\nfrom langchain_openai import OpenAI\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os\\n\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\n\\n\\napp = Flask(__name__)\\n\\nPINECONE_API_KEY = os.environ.get(\\'PINECONE_API_KEY\\')\\nOPENAI_API_KEY = os.environ.get(\\'OPENAI_API_KEY\\')\\n\\nos.environ[\\'PINECONE_API_KEY\\'] = PINECONE_API_KEY\\nos.environ[\\'OPENAI_API_KEY\\'] = OPENAI_API_KEY\\n\\nembedding = download_huggingface_embedding()\\n \\n## loading existing index from pinecone database \\n\\nindex_name = \"medicalbot\"\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name = index_name,\\n    embedding = embedding,\\n)\\n\\nreteriver = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\\n\\nllm = OpenAI(temperature=0.1, max_tokens=500)\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n     ]\\n)\\n\\n\\nquestion_answering_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(reteriver, question_answering_chain)\\n\\n\\n\\n@app.route(\"/\")\\ndef index():\\n    return render_template(\"chat.html\")\\n\\n\\n@app.route(\"/get\", methods=[\"GET\",\"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response\", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\nif __name__ == \"__main__\":\\n    app.run(host=\"0.0.0.0\", port=8080, debug=True)\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from setuptools import find_packages, setup\\n\\nsetup(\\n    name = \"Generative AI Project\",\\n    version = \"0.0.0\",\\n    author= \"Rohit Mukati\",\\n    author_email= \"rohanmukati2002@gmail.com\",\\n    packages= find_packages(),\\n    install_requires = []\\n    \\n)', metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from src.helper import load_pdf_files, text_splitter, download_huggingface_embedding\\nfrom pinecone import ServerlessSpec\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nimport os\\n\\n\\nload_dotenv()\\nPINECONE_API_KEY = os.environ.get(\\'PINECONE_API_KEY\\')\\nos.environ[\\'PINECONE_API_KEY\\'] = PINECONE_API_KEY\\n\\nextracted_data = load_pdf_files()\\ntext_chunk = text_splitter(extracted_data)\\nembedding = download_huggingface_embedding()\\n\\n\\n## pinecone inilization\\npc = Pinecone(api_key=PINECONE_API_KEY)\\nindex_name = \"medicalbot\"\\n\\npc.create_index(\\n    name=index_name,\\n    dimension=384, # Replace with your model dimensions\\n    metric=\"cosine\", # Replace with your model metric\\n    spec=ServerlessSpec(\\n        cloud=\"aws\",\\n        region=\"us-east-1\"\\n    ) \\n)\\n\\n\\n## embedded each quey and chunk into pinecone VectorStores\\ndocssearch = PineconeVectorStore.from_documents(\\n    documents=text_chunk,\\n    index_name=index_name,\\n    embedding=embedding\\n)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\"\\n]\\n\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n    \\n    if filedir != \"\":\\n        os.makedirs(filedir, exist_ok = True)\\n        logging.info(f\"creating directory; {filedir} for the file: {filename}\")\\n        \\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"creating file: {filename}\")\\n            \\n    else:\\n        logging.info(f\"{filename} already exists\")\\n\\n\\n', metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n\\n\\n\\n\\ndef load_pdf_files():\\n    loader = PyPDFLoader(\"Data\\\\Medical_book.pdf\")\\n    documents = loader.load()\\n    \\n    return documents\\n\\n\\n\\n## Splitting data into chunks\\n\\ndef text_splitter(extracted_data):\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap=20 )\\n    text_chunks = text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n\\n## download the embedding from huggingface\\n\\ndef download_huggingface_embedding():\\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n    return embeddings\\n    \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='system_prompt = (\\n    \"\"\"\\n    You are Gopi, a professional chatbot designed primarily for medical and biological question-answering tasks. \\n    You were created by Rohit Mukati to assist users with accurate and concise medical guidance.\\n\\n    Use the following retrieved context to answer questions related to medical, biological, or disease-related topics. \\n    If a question falls outside these domains but pertains to basic general conversation, respond politely and professionally. \\n    For any complex or unrelated topics outside your scope, respond with: \\n    \"I\\'m sorry, I don\\'t know about that.\"\\n\\n    Keep your answers concise, clear, shorter and under three sentences.\\n\\n    Context for the question:\\n    \"{context}\"\\n    \"\"\"\\n)\\n', metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_huggingface_embedding\\nfrom langchain_openai import OpenAI\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os\\n\\nfrom dotenv import load_dotenv\\nload_dotenv()\\n\\n\\n\\napp = Flask(__name__)\\n\\nPINECONE_API_KEY = os.environ.get(\\'PINECONE_API_KEY\\')\\nOPENAI_API_KEY = os.environ.get(\\'OPENAI_API_KEY\\')\\n\\nos.environ[\\'PINECONE_API_KEY\\'] = PINECONE_API_KEY\\nos.environ[\\'OPENAI_API_KEY\\'] = OPENAI_API_KEY\\n\\nembedding = download_huggingface_embedding()\\n \\n## loading existing index from pinecone database \\n\\nindex_name = \"medicalbot\"\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name = index_name,\\n    embedding = embedding,\\n)\\n\\nreteriver = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\\n\\nllm = OpenAI(temperature=0.1, max_tokens=500)\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n     ]\\n)\\n\\n\\nquestion_answering_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(reteriver, question_answering_chain)\\n\\n\\n\\n@app.route(\"/\")\\ndef index():\\n    return render_template(\"chat.html\")\\n\\n\\n@app.route(\"/get\", methods=[\"GET\",\"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response\", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\nif __name__ == \"__main__\":\\n    app.run(host=\"0.0.0.0\", port=8080, debug=True)\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_huggingface_embedding\\nfrom langchain_openai import OpenAI\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os\\n\\nfrom dotenv import load_dotenv\\nload_dotenv()', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"from dotenv import load_dotenv\\nload_dotenv()\\n\\n\\n\\napp = Flask(__name__)\\n\\nPINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\\nOPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\\n\\nos.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\\nos.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\\n\\nembedding = download_huggingface_embedding()\\n \\n## loading existing index from pinecone database\", metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='os.environ[\\'PINECONE_API_KEY\\'] = PINECONE_API_KEY\\nos.environ[\\'OPENAI_API_KEY\\'] = OPENAI_API_KEY\\n\\nembedding = download_huggingface_embedding()\\n \\n## loading existing index from pinecone database \\n\\nindex_name = \"medicalbot\"\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name = index_name,\\n    embedding = embedding,\\n)\\n\\nreteriver = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='reteriver = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\\n\\nllm = OpenAI(temperature=0.1, max_tokens=500)\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n     ]\\n)\\n\\n\\nquestion_answering_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(reteriver, question_answering_chain)\\n\\n\\n\\n@app.route(\"/\")', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='def index():\\n    return render_template(\"chat.html\")\\n\\n\\n@app.route(\"/get\", methods=[\"GET\",\"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response\", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\nif __name__ == \"__main__\":\\n    app.run(host=\"0.0.0.0\", port=8080, debug=True)', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from setuptools import find_packages, setup\\n\\nsetup(\\n    name = \"Generative AI Project\",\\n    version = \"0.0.0\",\\n    author= \"Rohit Mukati\",\\n    author_email= \"rohanmukati2002@gmail.com\",\\n    packages= find_packages(),\\n    install_requires = []\\n    \\n)', metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"from src.helper import load_pdf_files, text_splitter, download_huggingface_embedding\\nfrom pinecone import ServerlessSpec\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nimport os\\n\\n\\nload_dotenv()\\nPINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\\nos.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\", metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='load_dotenv()\\nPINECONE_API_KEY = os.environ.get(\\'PINECONE_API_KEY\\')\\nos.environ[\\'PINECONE_API_KEY\\'] = PINECONE_API_KEY\\n\\nextracted_data = load_pdf_files()\\ntext_chunk = text_splitter(extracted_data)\\nembedding = download_huggingface_embedding()\\n\\n\\n## pinecone inilization\\npc = Pinecone(api_key=PINECONE_API_KEY)\\nindex_name = \"medicalbot\"', metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='## pinecone inilization\\npc = Pinecone(api_key=PINECONE_API_KEY)\\nindex_name = \"medicalbot\"\\n\\npc.create_index(\\n    name=index_name,\\n    dimension=384, # Replace with your model dimensions\\n    metric=\"cosine\", # Replace with your model metric\\n    spec=ServerlessSpec(\\n        cloud=\"aws\",\\n        region=\"us-east-1\"\\n    ) \\n)', metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='## embedded each quey and chunk into pinecone VectorStores\\ndocssearch = PineconeVectorStore.from_documents(\\n    documents=text_chunk,\\n    index_name=index_name,\\n    embedding=embedding\\n)', metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\"\\n]', metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='for filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n    \\n    if filedir != \"\":\\n        os.makedirs(filedir, exist_ok = True)\\n        logging.info(f\"creating directory; {filedir} for the file: {filename}\")\\n        \\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"creating file: {filename}\")\\n            \\n    else:', metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"creating file: {filename}\")\\n            \\n    else:\\n        logging.info(f\"{filename} already exists\")', metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n\\n\\n\\n\\ndef load_pdf_files():\\n    loader = PyPDFLoader(\"Data\\\\Medical_book.pdf\")\\n    documents = loader.load()\\n    \\n    return documents\\n\\n\\n\\n## Splitting data into chunks', metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='def load_pdf_files():\\n    loader = PyPDFLoader(\"Data\\\\Medical_book.pdf\")\\n    documents = loader.load()\\n    \\n    return documents\\n\\n\\n\\n## Splitting data into chunks\\n\\ndef text_splitter(extracted_data):\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap=20 )\\n    text_chunks = text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n\\n## download the embedding from huggingface', metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='def download_huggingface_embedding():\\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n    return embeddings', metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='system_prompt = (\\n    \"\"\"\\n    You are Gopi, a professional chatbot designed primarily for medical and biological question-answering tasks. \\n    You were created by Rohit Mukati to assist users with accurate and concise medical guidance.', metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='Use the following retrieved context to answer questions related to medical, biological, or disease-related topics. \\n    If a question falls outside these domains but pertains to basic general conversation, respond politely and professionally. \\n    For any complex or unrelated topics outside your scope, respond with: \\n    \"I\\'m sorry, I don\\'t know about that.\"\\n\\n    Keep your answers concise, clear, shorter and under three sentences.\\n\\n    Context for the question:\\n    \"{context}\"\\n    \"\"\"\\n)', metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = Chroma.from_documents(texts, embedding=embedding, persist_directory='.DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## llm\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectors.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":8}), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can you write me function of adding two numbers in python?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have that information.\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
